{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Classification is the prototype of the function that each team must develop to classify new data\n",
    "#This function must handle all the operation to: read in a streaming order the input file, make the earlier possibile classification, return the required data\n",
    "#Input: \n",
    "# - Folder Name: The name of the folder where the experiment file is stored\n",
    "# - Experiment: The experiment name that must be read \n",
    "#Output:\n",
    "# - Predicted Label: the label predicted by the classifier\n",
    "# - Time for classification: how much time of the input data was required to perform the classification task\n",
    "# - Ranking: The Features ranked according to the team solution\n",
    "\n",
    "def TestClassification(FolderName,Experiment):\n",
    "\n",
    "        \n",
    "    Label = \"\"\n",
    "    Time = -1\n",
    "    Ranking = []\n",
    "\n",
    "    \n",
    "    #Output example to run the script\n",
    "    Label=0\n",
    "    Time = 10\n",
    "    Ranking=[\"S4\",\"S2\",\"S1\",\"S3\"]\n",
    "\n",
    "    return Label, Time, Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Record Performance function store the information of the performance achieved in the 1st run of the classification\n",
    "def RecordPerformance(Experiment, Label, Time, Ranking):\n",
    "\n",
    "    if not os.path.exists('First'):\n",
    "        os.makedirs('First')\n",
    "    \n",
    "    PerformanceOutput = open(\"First/%s.csv\"%Experiment,\"w\")\n",
    "    PerformanceOutput.write(\"Experiment;Label;Time;Ranking\\n\")\n",
    "    PerformanceOutput.write(Experiment+\";\"+str(Label)+\";\"+str(Time)+\";\"+str(Ranking)+\"\\n\")\n",
    "    PerformanceOutput.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "#The CutExperiment function is used to cut the input experiment in the time order. For each experiment, the cut is performed accoring to the time to classification declared by the team \n",
    "def CutExperiment(FolderName, Experiment, Time):\n",
    "\n",
    "    if not os.path.exists('Cut'):\n",
    "        os.makedirs('Cut')\n",
    "    \n",
    "    data = pd.read_csv(FolderName+\"/%s.csv\"%Experiment,sep=\",\")  \n",
    "\n",
    "    df = pd.DataFrame(columns = [\"c1\",\"c2\"])\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        field = data.iloc[i][0]\n",
    "        records = eval(data.iloc[i][1])[:Time]\n",
    "\n",
    "        df = df.append({\"c1\": field, \"c2\": records}, ignore_index=True)    \n",
    "    \n",
    "    columns =list(data.columns)\n",
    "    df.columns = [\"\",columns[1]]\n",
    "    df.to_csv(\"Cut/%s.csv\"%Experiment,index=False)\n",
    "    return\n",
    "\n",
    "#ComparePerfomance check if the team achieved the same performance in the 1st and in the 2nd run \n",
    "def ComparePerformance(Experiment,Label, Time, Ranking):\n",
    "    \n",
    "    Performance = pd.read_csv(\"First/%s.csv\"%Experiment,sep=\";\")        \n",
    "        \n",
    "    if(Performance[\"Label\"].iloc[0]!=Label): return False\n",
    "    if(Performance[\"Time\"].iloc[0]!=Time): return False\n",
    "    if(Performance[\"Ranking\"].iloc[0]!=str(Ranking)): return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "#GetWorst function returns the worst performance in case the 1st and 2nd run performance does not match\n",
    "def GetWorst(Experiment, Label, Time, Ranking):\n",
    "    \n",
    "    Performance = pd.read_csv(\"First/%s.csv\"%Experiment,sep=\";\")  \n",
    "    \n",
    "    if(Performance[\"Label\"].iloc[0]!=Label): return Performance[\"Label\"].iloc[0], -1, Performance[\"Ranking\"].iloc[0]\n",
    "    if(Performance[\"Time\"].iloc[0]!=Time): return Performance[\"Label\"].iloc[0], Performance[\"Time\"].iloc[0], Performance[\"Ranking\"].iloc[0]\n",
    "    if(Performance[\"Ranking\"].iloc[0]!=str(Ranking)): return Performance[\"Label\"].iloc[0], Performance[\"Time\"].iloc[0], Performance[\"Ranking\"].iloc[0]\n",
    "    \n",
    "    return\n",
    "\n",
    "#Logperformance function stores the final performance. Only this performance will be used to compute the Penalty score of each team\n",
    "def LogPerformance(Experiment,Label, Time, Ranking):\n",
    "\n",
    "    if not os.path.exists('Results'):\n",
    "        os.makedirs('Results')\n",
    "        \n",
    "    PerformanceOutput = open(\"Results/%s.csv\"%Experiment,\"w\")\n",
    "    PerformanceOutput.write(\"Experiment;Label;Time;Ranking\\n\")\n",
    "    PerformanceOutput.write(Experiment+\";\"+str(Label)+\";\"+str(Time)+\";\"+str(Ranking)+\"\\n\")\n",
    "    PerformanceOutput.close()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of the validation pipleline by using a single experiment.\n",
    "#Data/ is the folder where the experiment is stored\n",
    "#class_0_0_data is the experiment name\n",
    "#Cut/ is the folder where only the cut experiment will be saved\n",
    "\n",
    "def main():\n",
    "    \n",
    "    FolderName = \"Data/\"\n",
    "    Experiment = \"class_0_0_data\"\n",
    "    Label, Time, Ranking = TestClassification(FolderName,Experiment)\n",
    "    \n",
    "    RecordPerformance(Experiment, Label, Time, Ranking)\n",
    "    CutExperiment(FolderName,Experiment,Time)\n",
    "    \n",
    "    FolderName = \"Cut/\"\n",
    "    Label, Time, Ranking = TestClassification(FolderName,Experiment)\n",
    "    \n",
    "    Equal = ComparePerformance(Experiment,Label, Time, Ranking)\n",
    "    if(Equal==False):\n",
    "        Label, Time, Ranking = GetWorst(Experiment,Label, Time, Ranking)\n",
    "    \n",
    "    LogPerformance(Experiment,Label, Time, Ranking)\n",
    "    return\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
